[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/08wk.html",
    "href": "posts/08wk.html",
    "title": "08wk: 예측 모델 훈련과 선택 - 교차 검증",
    "section": "",
    "text": "데이터 출처\n\nhttps://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset/data\n\n\n\nimport pandas as pd\nimport numpy as np\n\n\ndata = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\ndata = data.drop(['id'], axis=1)\ndata = data[data['gender'] != 'Other']\ndata.head()\n\ndf = data.copy()\ndf.loc[:, [\"hypertension\", \"heart_disease\", \"stroke\"]] = data.loc[:, [\"hypertension\", \"heart_disease\", \"stroke\"]].applymap(lambda x: \"Yes\" if x == 1 else \"No\")\ndf.head()\n\n\n\n\n\n\n\n\ngender\nage\nhypertension\nheart_disease\never_married\nwork_type\nResidence_type\navg_glucose_level\nbmi\nsmoking_status\nstroke\n\n\n\n\n0\nMale\n67.0\nNo\nYes\nYes\nPrivate\nUrban\n228.69\n36.6\nformerly smoked\nYes\n\n\n1\nFemale\n61.0\nNo\nNo\nYes\nSelf-employed\nRural\n202.21\nNaN\nnever smoked\nYes\n\n\n2\nMale\n80.0\nNo\nYes\nYes\nPrivate\nRural\n105.92\n32.5\nnever smoked\nYes\n\n\n3\nFemale\n49.0\nNo\nNo\nYes\nPrivate\nUrban\n171.23\n34.4\nsmokes\nYes\n\n\n4\nFemale\n79.0\nYes\nNo\nYes\nSelf-employed\nRural\n174.12\n24.0\nnever smoked\nYes\n\n\n\n\n\n\n\n\n\n\n결측치 처리, 표준화, 인코딩\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop([\"stroke\"], axis=1)\ny = LabelEncoder().fit_transform(df['stroke'])\n\n\nfrom sklearn.impute import SimpleImputer\n\nX_num = X.select_dtypes(include = 'number')\nX_cat = X.select_dtypes(exclude = 'number')\n\nX[X_num.columns] = SimpleImputer(strategy=\"mean\").fit_transform(X_num)\nX[X_cat.columns] = SimpleImputer(strategy=\"most_frequent\").fit_transform(X_cat)\n\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nscaler = StandardScaler()\n\nonehot = OneHotEncoder(drop = 'first', handle_unknown='ignore', sparse_output=False)\n\nct = ColumnTransformer([('scaler', scaler, X_num.columns),\n                        ('onehot', onehot, X_cat.columns)], \n                       remainder='passthrough', n_jobs=-1)\n\nct\n\nColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformerColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])scalerIndex(['age', 'avg_glucose_level', 'bmi'], dtype='object')StandardScalerStandardScaler()onehotIndex(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object')OneHotEncoderOneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)remainderpassthroughpassthrough"
  },
  {
    "objectID": "posts/08wk.html#a.-전처리",
    "href": "posts/08wk.html#a.-전처리",
    "title": "08wk: 예측 모델 훈련과 선택 - 교차 검증",
    "section": "",
    "text": "결측치 처리, 표준화, 인코딩\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop([\"stroke\"], axis=1)\ny = LabelEncoder().fit_transform(df['stroke'])\n\n\nfrom sklearn.impute import SimpleImputer\n\nX_num = X.select_dtypes(include = 'number')\nX_cat = X.select_dtypes(exclude = 'number')\n\nX[X_num.columns] = SimpleImputer(strategy=\"mean\").fit_transform(X_num)\nX[X_cat.columns] = SimpleImputer(strategy=\"most_frequent\").fit_transform(X_cat)\n\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nscaler = StandardScaler()\n\nonehot = OneHotEncoder(drop = 'first', handle_unknown='ignore', sparse_output=False)\n\nct = ColumnTransformer([('scaler', scaler, X_num.columns),\n                        ('onehot', onehot, X_cat.columns)], \n                       remainder='passthrough', n_jobs=-1)\n\nct\n\nColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformerColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])scalerIndex(['age', 'avg_glucose_level', 'bmi'], dtype='object')StandardScalerStandardScaler()onehotIndex(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object')OneHotEncoderOneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)remainderpassthroughpassthrough"
  },
  {
    "objectID": "posts/08wk.html#a.-stratifiedkfold",
    "href": "posts/08wk.html#a.-stratifiedkfold",
    "title": "08wk: 예측 모델 훈련과 선택 - 교차 검증",
    "section": "A. StratifiedKFold()",
    "text": "A. StratifiedKFold()\n\nkfold = StratifiedKFold(n_splits=5).split(X_train, y_train)\n\nscores = []\n\nfor k, (train, test) in enumerate(kfold):\n    predictr.fit(ct.fit_transform(X_train.iloc[train]), y_train[train])\n    score = predictr.score(ct.transform(X_train.iloc[test]), y_train[test])\n    scores.append(score)\n\n    print(f'폴드: {k+1:02d}, '\n          f'클래스 분포: {np.bincount(y_train[train])}, '\n          f'정확도: {score:.4f}')\n\nmean_acc = np.mean(scores)\nstd_acc = np.std(scores)\nprint(f'\\nCV 정확도: {mean_acc:.4f} +/- {std_acc:.4f}')\n\n폴드: 01, 클래스 분포: [3110  159], 정확도: 0.9511\n폴드: 02, 클래스 분포: [3110  159], 정확도: 0.9511\n폴드: 03, 클래스 분포: [3110  160], 정확도: 0.9523\n폴드: 04, 클래스 분포: [3111  159], 정확도: 0.9510\n폴드: 05, 클래스 분포: [3111  159], 정확도: 0.9510\n\nCV 정확도: 0.9513 +/- 0.0005\n\n\n\ncross_validate()\n\n\nimport time\n\nstart_time = time.time()\n\n# estimator = pipe에 predictr_gs 수행 시 반올림 하기 전 파라미터로 수행. (시간이 더 소요)\nscores = cross_validate(estimator = pipe,\n                        X=X_train,\n                        y=y_train,\n                        scoring = ['accuracy', 'roc_auc'], # 'accuracy', 'precision', 'recall', 'f1', 'roc_auc'\n                        cv = StratifiedKFold(n_splits=5),\n                        n_jobs = -1)\n\naccuracy_score_gs = np.mean(scores['test_accuracy']).round(4);accuracy_score_gs\n\nend_time = time.time()\nprint(\"코드 실행 시간: {:.1f} 초\".format(end_time - start_time))\n\n코드 실행 시간: 0.8 초\n\n\n\naccuracy_score_gs\n\n0.9513"
  },
  {
    "objectID": "posts/04wk.html",
    "href": "posts/04wk.html",
    "title": "04wk: 전처리: 데이터 형태 갖추기 - scaler",
    "section": "",
    "text": "의사나무결정 기반 알고리즘은 스케일 조정에 영향을 받지 않음.\n경사하강법 알고리즘을 구현하는 대부분의 머신 러닝과 최적화 알고리즘은 특성의 스케일이 같을 때 훨씬 성능이 좋아짐.\n단위가 다른 변수들의 중요도를 더 쉽게 파악하기 위해 모든 변수를 동일한 기준으로 전처리.\n\n- 이유. - 예를들어 첫 번째 특성이 1에서 10 사이 스케일을 가지고 있고 두 번째 특성은 1에서 10만 사이 스케일을 가진다고 가정 - 아달린에서 제곱 오차 함수를 이용한 알고리즘은 대부분 두 번째 특성의 큰 오차에 맞추어 가중치를 최적화. - 유클리드 거리 지표를 사용한 k-최근접 이웃인 경우 샘플 간의 거리를 계산하면 두 번째 특성 축에 좌우될 것.\n- 스케일 조정 대표적인 방법\n- 정규화 (normalization)\n- 표준화 (standardization)\nA. 정규화: - MinMaxScaling 변환의 특별한 경우 - 작동 원리: 데이터를 0과 1 사이의 값으로 조정 - 장점: 원하는 범위 내로 데이터를 조정할 때 유용. 특히 신경망에서는 활성화 함수의 범위와 일치하도록 입력 값을 조정하는 데 유용. [sigmoid, tanh와 같은 활성화 함수의 출력값과 맞추는 용도] - 단점: 이상치에 매우 민감하다. 이상치 때문에 전체 데이터의 스케일이 크게 영향받을 수 있음.\nB. 표준화: - StandardScaler - 작동 원리: 데이터의 평균을 0, 표준편차를 1로 만드는 방식으로 조정. - 장점: 가중치를 0 또는 0에 가까운 작은 난수로 초기화하여 가중치를 더 쉽게 학습 할 수 있음, 이상치에 MinMaxScaler보다 덜 민감함. 많은 통계적 기법들, 특히 PCA 같은 선형 알고리즘(경사 하강법 같은 최적화 알고리즘)에서 잘 작동함. - 단점: MinMaxScaler와 달리, 표준화된 데이터의 값이 특정 범위 내에 있음을 보장하지 않음.\n- 둘 중 어느 것을 선택할지 결정하기 위한 고려사항:\n\n이상치가 많으면 StandardScaler가 더 적합할 수 있다.\n모델의 알고리즘과 특성에 따라 선택해야 한다. 예를 들어, 신경망은 일반적으로 0과 1 사이의 값이나 -1과 1 사이의 값으로 입력을 받는 활성화 함수를 사용하므로 MinMaxScaler가 적합할 수 있다.\n\n결론적으로, 두 스케일링 방법 중 어느 것이 더 좋은지는 사용 사례와 데이터의 특성에 따라 다르기 때문에, 가능한 경우 둘 다 시도해보고 모델의 성능을 비교하는 것이 좋다."
  },
  {
    "objectID": "posts/04wk.html#a.-표준화",
    "href": "posts/04wk.html#a.-표준화",
    "title": "04wk: 전처리: 데이터 형태 갖추기 - scaler",
    "section": "A. 표준화",
    "text": "A. 표준화\n\nfrom sklearn.preprocessing import LabelEncoder\nX = df.drop([\"stroke\"], axis=1)\ny = LabelEncoder().fit_transform(df['stroke'])\n\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nscaler = StandardScaler()\n\nonehot = OneHotEncoder(drop = 'first', handle_unknown='ignore', sparse_output=False)\n\nct = ColumnTransformer([('scaler', scaler, X.select_dtypes(include = \"number\").columns),\n                        ('onehot', onehot, X.select_dtypes(exclude = \"number\").columns)], \n                       remainder='passthrough', n_jobs=-1)\nct\n\nColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['id', 'age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformerColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['id', 'age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])scalerIndex(['id', 'age', 'avg_glucose_level', 'bmi'], dtype='object')StandardScalerStandardScaler()onehotIndex(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object')OneHotEncoderOneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)remainderpassthroughpassthrough\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\n참고\n앞 장에서 배운 범주형 인코딩도 같이 적용.\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n\nct.fit_transform(X_train)\n\narray([[-0.5304543 ,  0.20566087, -0.8199733 , ...,  0.        ,\n         1.        ,  0.        ],\n       [ 0.33840517, -1.25490055,  0.35207477, ...,  0.        ,\n         1.        ,  0.        ],\n       [ 0.49752263,  1.04659017,  0.09066209, ...,  0.        ,\n         1.        ,  0.        ],\n       ...,\n       [-1.21472057,  0.78103355, -0.61137349, ...,  1.        ,\n         0.        ,  0.        ],\n       [-1.5443042 , -0.54674956, -0.71302171, ...,  0.        ,\n         0.        ,  0.        ],\n       [-0.22187286, -1.65323548, -0.33736527, ...,  0.        ,\n         0.        ,  0.        ]])\n\n\n\nct.transform(X_test)\n\narray([[-0.5813587 ,  0.86955242, -0.62065442, ...,  0.        ,\n         1.        ,  0.        ],\n       [ 1.38719593, -0.01563631, -0.43415205, ...,  0.        ,\n         1.        ,  0.        ],\n       [ 0.73184298, -0.90082505,  0.44974544, ...,  0.        ,\n         0.        ,  1.        ],\n       ...,\n       [-0.43161218,  0.29417974, -0.31305809, ...,  0.        ,\n         1.        ,  0.        ],\n       [-0.01618329, -1.25490055, -0.96670029, ...,  0.        ,\n         1.        ,  0.        ],\n       [-0.92671751,  0.33843918, -0.50862041, ...,  1.        ,\n         0.        ,  0.        ]])\n\n\n\n표준화 공식\n\n\nct.fit_transform(X_train)[:,0][:5]\n\narray([-0.5304543 ,  0.33840517,  0.49752263,  0.96291411, -0.48138641])\n\n\n\nex = X_train.iloc[:,0].values\nprint('standardized:', ((ex - ex.mean()) / ex.std(ddof=0))[:5])\n\nstandardized: [-0.5304543   0.33840517  0.49752263  0.96291411 -0.48138641]\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\n참고\nscaler은 훈련 셋으로만 학습하고 학습된 기준으로 테스트 셋 변환.\n- 스케일 변환 .fit()와 .fit_transform()은 입력으로 2차원 자료구조를 기대한다. (그중에서도 은근히 numpy array를 기대함)"
  },
  {
    "objectID": "posts/04wk.html#b.-정규화",
    "href": "posts/04wk.html#b.-정규화",
    "title": "04wk: 전처리: 데이터 형태 갖추기 - scaler",
    "section": "B. 정규화",
    "text": "B. 정규화\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n\n# step 1\nmmsclr = MinMaxScaler()\n\n# step 2\nX_train_norm = mmsclr.fit_transform(X_train)\n\n# step 3\nX_test_norm = mmsclr.transform(X_test)\n\n\nX_train_norm[:, -1][:5]\n\narray([0.19400856, 0.68259629, 0.71825963, 0.58273894, 0.29743224])\n\n\n\nex = X_train[:, -1]\nprint('normalized:', ((ex - ex.min()) / (ex.max() - ex.min()))[:5])\n\nnormalized: [0.19400856 0.68259629 0.71825963 0.58273894 0.29743224]\n\n\n\n# 원본화\nremmsclr = mmsclr.inverse_transform(X_train_norm).round(5)"
  },
  {
    "objectID": "posts/04wk.html#c.-다양한-scaler",
    "href": "posts/04wk.html#c.-다양한-scaler",
    "title": "04wk: 전처리: 데이터 형태 갖추기 - scaler",
    "section": "C. 다양한 scaler",
    "text": "C. 다양한 scaler\n\n1). RobustScaler()\n\nfrom sklearn.preprocessing import RobustScaler\n\n\n특성 열마다 독립적으로 작용하며 각 값을 중간 값으로 뺀 다음 (데이터셋의 3사분위수와 - 1사분위수)로 나눠서 데이터셋의 스케일 조정.\n\n이상치가 많이 포함된 작은 데이터셋을 다룰 때 특히 도움이 됨.\n또환 과대적합이 되기 쉽다면 RobustScaler가 좋은 선택.\n극단적인 값과 이상치에 영향을 덜 받음.\n\n\n\n# step 1\nrbsclr = RobustScaler()\n\n# step 2\nX_train_robust = rbsclr.fit_transform(X_train)\n\n# step 3\nX_test_robust = rbsclr.transform(X_test)\n\n\nX_train_robust[:, -1][:5]\n\narray([-0.23094904,  1.05002338,  1.14352501,  0.78821879,  0.0402057 ])\n\n\n\nex = X_train[:, -1]\nprint('robust:', ((ex - np.percentile(ex, 50)) / (np.percentile(ex, 75) - np.percentile(ex, 25)))[:5])\n\nrobust: [-0.23094904  1.05002338  1.14352501  0.78821879  0.0402057 ]\n\n\n\n\n2) MaxAbsScaler()\n\nfrom sklearn.preprocessing import MaxAbsScaler\n\n\n각 특성별로 데이터를 최대 절댓값으로 나눔. 따라서 각 특성의 최댓값은 1이며 [-1,1] 범위.\n데이터를 중앙에 맞추지 않으므로 희소행렬 사용 가능\n\n\n# step 1\nmasclr = MaxAbsScaler()\n\n# step 2\nX_train_maxabs = masclr.fit_transform(X_train)\n\n# step 3\nX_test_maxabs = masclr.transform(X_test)\n\n\nX_train_maxabs[:, -1][:5]\n\n\nex = X_train[:, -1]\nprint('MaxAbsScaler:', (ex / np.max(np.abs(ex)))[:5])\n\n\n\n3) Normalizer()\n\nfrom sklearn.preprocessing import Normalizer\n\n\n특성별이 아닌 샘플별로 정규화 수행. 또한 희소 행렬도 처리 가능. 기본적으로 각 샘플의 L2노름이 1이 되도록 정규화.\n\n\n# step 1\nnrmsclr = Normalizer() # default: norm = 'l2', 'l1', 'max' \n\n# step 2\nX_train_l2 = nrmsclr.fit_transform(X_train)\n\n# step 3\nX_test_l2 = nrmsclr.transform(X_test)\n\n\nX_train_l2[0, :].round(3)\n\n\n원래 특성을 제곱한 행을 하나 더 추가. (0 나눗셈 오류를 다루기 번거로우므로 편의상 0 제거)\n\n\n# Normalizer는 행 기준이므로 바꿔줌\nex = X_train[0,:]\n\n\nex_2f = np.vstack((ex, ex**2))\nex_2f.shape\n\n\nL2: 샘플별 특성의 제곱 합을 제곱근으로 나눔\n\n\nl2_norm = np.sqrt(np.sum(ex_2f ** 2, axis=1))\nprint(l2_norm)\n(ex_2f / l2_norm.reshape(-1, 1))[0].round(3)\n\n\nL1: 샘플별 특성의 절댓값 합으로 나눔\n\n\n# step 1\nnrmsclr = Normalizer(norm = 'l1') # default: norm = 'l2', 'l1', 'max' \n\n# step 2\nX_train_l1 = nrmsclr.fit_transform(X_train)\nX_train_l1[0, :].round(3)\n\n\nl1_norm = np.sum(np.abs(ex_2f), axis=1)\nprint(l1_norm)\n(ex_2f / l1_norm.reshape(-1, 1))[0].round(3)\n\n\nmax: 각 샘플의 최대 절댓값으로 나눔\n\n\n# step 1\nnrmsclr = Normalizer(norm = 'max') # default: norm = 'l2', 'l1', 'max' \n\n# step 2\nX_train_max = nrmsclr.fit_transform(X_train)\nX_train_max[0, :].round(3)\n\n\nmax_norm = np.max(np.abs(ex_2f), axis=1)\nprint(max_norm)\n(ex_2f / max_norm.reshape(-1, 1))[0].round(3)"
  },
  {
    "objectID": "posts/10wk.html",
    "href": "posts/10wk.html",
    "title": "10wk: 예측 모델 훈련과 선택 - 각 모델 튜닝 범위",
    "section": "",
    "text": "Logistic Regression - L1, L2\n\nparam_grid = {\n    'Lasso__C': np.logspace(-4, 4, 10)\n}\n\n\nparam_dist = {\n    'Lasso__C': scipy.stats.loguniform(1e-4, 1e4)\n}\n\n\nsearch_spaces = {\n    'Lasso__C': (0.001, 1000, 'log-uniform') # scipy.stats.loguniform(1e-4, 1e4)\n}\n\n\n\nLogistic Regression - ElasticNet\n\nparam_grid = {\n    'ElasticNet__C': np.logspace(-4, 4, num=10),\n    'ElasticNet__l1_ratio': np.arange(0.1, 0.9, 0.1)\n}\n\n\nparam_dist = {\n    'ElasticNet__C': scipy.stats.loguniform(1e-4, 1e4),\n    'ElasticNet__l1_ratio': scipy.stats.uniform(0.1, 0.8)\n}\n\n\nsearch_spaces = {\n    'ElasticNet__C': (0.001, 1000, 'log-uniform'),\n    'ElasticNet__l1_ratio': (0.1, 0.9)\n}\n\n\n\nRandomForest\n\nparam_grid = {\n    'RandomForest__n_estimators': (50, 100, 150),\n    'RandomForest__max_depth': (4, 6, 8),\n    'RandomForest__max_features': (0.4, 0.6, 0.8),\n    'RandomForest__min_samples_leaf' : (8, 12, 16),\n    'RandomForest__min_samples_split' : (8, 12, 16)\n}\n\n\nparam_dist = {\n    'RandomForest__n_estimators': scipy.stats.randint(50, 150),\n    'RandomForest__max_depth': scipy.stats.randint(4, 8),\n    'RandomForest__max_features': scipy.stats.uniform(0.4, 0.4), # 0.8 직전까지 가능\n    'RandomForest__min_samples_leaf' : scipy.stats.randint(8, 16),\n    'RandomForest__min_samples_split' : scipy.stats.randint(8, 16)\n}\n\n\nsearch_spaces = {\n    'RandomForest__n_estimators': (50, 150),\n    'RandomForest__max_depth': (4, 8),\n    'RandomForest__max_features': (0.4, 0.8),\n    'RandomForest__min_samples_leaf' : (8, 16),\n    'RandomForest__min_samples_split' : (8, 16)\n}\n\n\n\nXGBoost\n\nparam_grid = {\n    'XGBoost__learning_rate': (0.01, 0.05, 0.1, 0.15, 0.2),\n    'XGBoost__n_estimators': (50, 100, 150),\n    'XGBoost__max_depth': (4, 6, 8),\n    'XGBoost__colsample_bytree': (0.4, 0.6, 0.8),\n    'XGBoost__gamma' : (3,6),\n    'XGBoost__min_child_weight' : (8, 12, 16),\n    'XGBoost__subsample' : (0.4, 0.6, 0.8),\n}\n\n\nparam_dist = {\n    'XGBoost__learning_rate': scipy.stats.uniform(0.01, 0.19),\n    'XGBoost__n_estimators': scipy.stats.randint(50, 150),\n    'XGBoost__max_depth': scipy.stats.randint(4, 8),\n    'XGBoost__colsample_bytree': scipy.stats.uniform(0.4, 0.4),\n    'XGBoost__gamma' : scipy.stats.randint(3, 6),\n    'XGBoost__min_child_weight' : scipy.stats.randint(8, 16),\n    'XGBoost__subsample' : scipy.stats.uniform(0.4, 0.4),\n}\n\n\nsearch_spaces = {\n    'XGBoost__learning_rate': (0.01, 0.2),\n    'XGBoost__n_estimators': (50, 150),\n    'XGBoost__max_depth': (4, 8),\n    'XGBoost__colsample_bytree': (0.4, 0.6),\n    'XGBoost__gamma' : (3, 6),\n    'XGBoost__min_child_weight' : (8, 16),\n    'XGBoost__subsample' : (0.4, 0.8)\n}\n\n\n\nMLP\n\nparam_grid = {\n    'MLP__alpha': (0.001, 0.01, 0.1, 1, 10),\n    'MLP__learning_rate_init' : (0.0001, 0.001, 0.01, 0.1, 0.2)\n}\n\n\nparam_dist = {\n    'MLP__alpha': scipy.stats.uniform(0.001, 9.999),\n    'MLP__learning_rate_init' : scipy.stats.uniform(0.0001, 0.1999)\n}\n\n\nsearch_spaces = {\n    'MLP__alpha': (0.001, 10),\n    'MLP__learning_rate_init' : (0.0001, 0.01)\n}\n\n\n\nSVC\n\nparam_dist_SVC = {\n    'SVC__C': scipy.stats.loguniform(1e-3, 1e2),\n    'SVC__gamma': scipy.stats.loguniform(1e-3, 1e2)\n}"
  },
  {
    "objectID": "posts/15wk.html",
    "href": "posts/15wk.html",
    "title": "15wk: 예측 - XGBoost-earlystopping",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_validate, cross_val_predict\nfrom sklearn.utils import resample\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nimport scipy.stats\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Integer, Categorical\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nimport time\n\n\nData\n\n데이터 출처\n\nhttps://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset/data\n\n\n\ndata = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\ndata = data.drop(['id'], axis=1)\ndata = data[data['gender'] != 'Other']\ndata.head()\n\ndf = data.copy()\ndf.loc[:, [\"hypertension\", \"heart_disease\", \"stroke\"]] = data.loc[:, [\"hypertension\", \"heart_disease\", \"stroke\"]].applymap(lambda x: \"Yes\" if x == 1 else \"No\")\ndf.head()\n\n\n\n\n\n\n\n\ngender\nage\nhypertension\nheart_disease\never_married\nwork_type\nResidence_type\navg_glucose_level\nbmi\nsmoking_status\nstroke\n\n\n\n\n0\nMale\n67.0\nNo\nYes\nYes\nPrivate\nUrban\n228.69\n36.6\nformerly smoked\nYes\n\n\n1\nFemale\n61.0\nNo\nNo\nYes\nSelf-employed\nRural\n202.21\nNaN\nnever smoked\nYes\n\n\n2\nMale\n80.0\nNo\nYes\nYes\nPrivate\nRural\n105.92\n32.5\nnever smoked\nYes\n\n\n3\nFemale\n49.0\nNo\nNo\nYes\nPrivate\nUrban\n171.23\n34.4\nsmokes\nYes\n\n\n4\nFemale\n79.0\nYes\nNo\nYes\nSelf-employed\nRural\n174.12\n24.0\nnever smoked\nYes\n\n\n\n\n\n\n\n\nX = df.drop([\"stroke\"], axis=1)\ny = LabelEncoder().fit_transform(df['stroke'])\n\nX_num = X.select_dtypes(include = 'number')\nX_cat = X.select_dtypes(exclude = 'number')\n\nX[X_num.columns] = SimpleImputer(strategy=\"mean\").fit_transform(X_num)\nX[X_cat.columns] = SimpleImputer(strategy=\"most_frequent\").fit_transform(X_cat)\n\n\nscaler = StandardScaler()\n\nonehot = OneHotEncoder(drop = 'first', handle_unknown='ignore', sparse_output=False)\n\nct = ColumnTransformer([('scaler', scaler, X_num.columns),\n                        ('onehot', onehot, X_cat.columns)], \n                       remainder='passthrough', n_jobs=-1)\n\nct\n\nColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformerColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])scalerIndex(['age', 'avg_glucose_level', 'bmi'], dtype='object')StandardScalerStandardScaler()onehotIndex(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object')OneHotEncoderOneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)remainderpassthroughpassthrough\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n\nct.fit(X_train)\n\nColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['Age', 'eGFR', 'SBP', 'DBP', 'HbA1c', 'Creatinine', 'Hemoglobin'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['Sex', 'Hypertension', 'Dyslipidemia', 'Heart', 'Stroke',\n       'Diabetic medicine', 'Insulin', 'Arb Acei', 'Diuretics', 'Statin'],\n      dtype='object'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformerColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['Age', 'eGFR', 'SBP', 'DBP', 'HbA1c', 'Creatinine', 'Hemoglobin'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['Sex', 'Hypertension', 'Dyslipidemia', 'Heart', 'Stroke',\n       'Diabetic medicine', 'Insulin', 'Arb Acei', 'Diuretics', 'Statin'],\n      dtype='object'))])scalerIndex(['Age', 'eGFR', 'SBP', 'DBP', 'HbA1c', 'Creatinine', 'Hemoglobin'], dtype='object')StandardScalerStandardScaler()onehotIndex(['Sex', 'Hypertension', 'Dyslipidemia', 'Heart', 'Stroke',\n       'Diabetic medicine', 'Insulin', 'Arb Acei', 'Diuretics', 'Statin'],\n      dtype='object')OneHotEncoderOneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)remainder[]passthroughpassthrough\n\n\n\n조기 중단(early_stopooing_rounds)\n\n검증 데이터셋의 손실 값이 증가하기 시작할 때(손실을 최소화하는 경우) 훈련을 중단하는 기법.\n따라서 모델을 훈련시키기 위해서는 두 개의 별도 데이터셋 필요\n\n모델 피팅을 위한 훈련 데이터\n손실 모니터링과 조기 중단을 위한 검증 데이터\n\n\nXgboost 알고리즘에서는 early_stoping_rounds 매개변수가 손실 값이 다음으로 감소할 때까지 기다릴 허용 범위를 제어.\n이 매개변수가 필요한 이유는 각 반복마다 손실 값이 무작위로 감소하기 때문입니다. 검증 손실은 일정 범위 내에서 변동할 수 있으며, 몇 번의 반복 후에 감소할 수 있습니다.\n\n일반적으로 조기 중단을 위해 50번의 반복을 사용하며, 보통 1000개의 트리와 함께 사용.\n전체 반복 횟수의 10%를 조기 중단에 사용하는 것이 합리적이라는 몇 가지 경험적인 법칙이 있습니다.\n\n\nhttps://mljar.com/blog/xgboost-early-stopping/\n\n# learning_rate, n_estimators, max_depth, min_child_weight, gamma, subsample, colsample_bytree, scale_pos_weight, early_stopping_rounds\n\npredictr = XGBClassifier(learning_rate = 0.1,\n                         n_estimators = 1000, \n                         max_depth = 3, \n                         min_child_weight = 3, \n                         gamma = 5, \n                         subsample = 0.5, \n                         colsample_bytree = 0.5, \n                         random_state = 42, \n                         scale_pos_weight = 1, \n                         eval_metric='logloss', \n                         early_stopping_rounds = 50)\n\n\nstart_time = time.time()\n\npredictr.fit(ct.fit_transform(X_train), y_train, eval_set = [(ct.fit_transform(X_train), y_train), (ct.transform(X_test), y_test)], verbose=2)\n\nend_time = time.time()\nprint(\"코드 실행 시간: {:.1f} 초\".format(end_time - start_time))\n\n[0] validation_0-logloss:0.61542    validation_1-logloss:0.61582\n[2] validation_0-logloss:0.49849    validation_1-logloss:0.49832\n[4] validation_0-logloss:0.41477    validation_1-logloss:0.41376\n[6] validation_0-logloss:0.35415    validation_1-logloss:0.35375\n[8] validation_0-logloss:0.30894    validation_1-logloss:0.30850\n[10]    validation_0-logloss:0.27531    validation_1-logloss:0.27517\n[12]    validation_0-logloss:0.24932    validation_1-logloss:0.24899\n[14]    validation_0-logloss:0.22942    validation_1-logloss:0.22914\n[16]    validation_0-logloss:0.21511    validation_1-logloss:0.21476\n[18]    validation_0-logloss:0.20194    validation_1-logloss:0.20147\n[20]    validation_0-logloss:0.19201    validation_1-logloss:0.19223\n[22]    validation_0-logloss:0.18444    validation_1-logloss:0.18521\n[24]    validation_0-logloss:0.17758    validation_1-logloss:0.17886\n[26]    validation_0-logloss:0.17243    validation_1-logloss:0.17448\n[28]    validation_0-logloss:0.16791    validation_1-logloss:0.17113\n[30]    validation_0-logloss:0.16564    validation_1-logloss:0.16954\n[32]    validation_0-logloss:0.16400    validation_1-logloss:0.16838\n[34]    validation_0-logloss:0.16320    validation_1-logloss:0.16734\n[36]    validation_0-logloss:0.16056    validation_1-logloss:0.16533\n[38]    validation_0-logloss:0.15846    validation_1-logloss:0.16500\n[40]    validation_0-logloss:0.15608    validation_1-logloss:0.16323\n[42]    validation_0-logloss:0.15435    validation_1-logloss:0.16213\n[44]    validation_0-logloss:0.15344    validation_1-logloss:0.16196\n[46]    validation_0-logloss:0.15268    validation_1-logloss:0.16116\n[48]    validation_0-logloss:0.15184    validation_1-logloss:0.16104\n[50]    validation_0-logloss:0.15087    validation_1-logloss:0.16108\n[52]    validation_0-logloss:0.15035    validation_1-logloss:0.16062\n[54]    validation_0-logloss:0.14999    validation_1-logloss:0.16069\n[56]    validation_0-logloss:0.14920    validation_1-logloss:0.16138\n[58]    validation_0-logloss:0.14886    validation_1-logloss:0.16152\n[60]    validation_0-logloss:0.14839    validation_1-logloss:0.16130\n[62]    validation_0-logloss:0.14797    validation_1-logloss:0.16114\n[64]    validation_0-logloss:0.14796    validation_1-logloss:0.16114\n[66]    validation_0-logloss:0.14771    validation_1-logloss:0.16111\n[68]    validation_0-logloss:0.14702    validation_1-logloss:0.16121\n[70]    validation_0-logloss:0.14687    validation_1-logloss:0.16092\n[72]    validation_0-logloss:0.14622    validation_1-logloss:0.16075\n[74]    validation_0-logloss:0.14622    validation_1-logloss:0.16075\n[76]    validation_0-logloss:0.14567    validation_1-logloss:0.16060\n[78]    validation_0-logloss:0.14567    validation_1-logloss:0.16061\n[80]    validation_0-logloss:0.14501    validation_1-logloss:0.16035\n[82]    validation_0-logloss:0.14469    validation_1-logloss:0.16064\n[84]    validation_0-logloss:0.14469    validation_1-logloss:0.16064\n[86]    validation_0-logloss:0.14451    validation_1-logloss:0.16104\n[88]    validation_0-logloss:0.14450    validation_1-logloss:0.16105\n[90]    validation_0-logloss:0.14451    validation_1-logloss:0.16104\n[92]    validation_0-logloss:0.14390    validation_1-logloss:0.16153\n[94]    validation_0-logloss:0.14350    validation_1-logloss:0.16223\n[96]    validation_0-logloss:0.14352    validation_1-logloss:0.16225\n[98]    validation_0-logloss:0.14352    validation_1-logloss:0.16228\n[100]   validation_0-logloss:0.14352    validation_1-logloss:0.16226\n[102]   validation_0-logloss:0.14352    validation_1-logloss:0.16224\n[104]   validation_0-logloss:0.14335    validation_1-logloss:0.16212\n[106]   validation_0-logloss:0.14334    validation_1-logloss:0.16214\n[108]   validation_0-logloss:0.14308    validation_1-logloss:0.16238\n[110]   validation_0-logloss:0.14308    validation_1-logloss:0.16238\n[112]   validation_0-logloss:0.14307    validation_1-logloss:0.16238\n[114]   validation_0-logloss:0.14289    validation_1-logloss:0.16258\n[116]   validation_0-logloss:0.14290    validation_1-logloss:0.16257\n[118]   validation_0-logloss:0.14290    validation_1-logloss:0.16257\n[120]   validation_0-logloss:0.14271    validation_1-logloss:0.16270\n[122]   validation_0-logloss:0.14270    validation_1-logloss:0.16271\n[124]   validation_0-logloss:0.14243    validation_1-logloss:0.16287\n[126]   validation_0-logloss:0.14243    validation_1-logloss:0.16287\n[128]   validation_0-logloss:0.14204    validation_1-logloss:0.16267\n[130]   validation_0-logloss:0.14204    validation_1-logloss:0.16268\n코드 실행 시간: 6.4 초\n\n\n\nprint('반복 횟수 : %d, 평가 지표 : %s' % (predictr.best_iteration, predictr.eval_metric))\n\n반복 횟수 : 81, 평가 지표 : logloss\n\n\n\nfrom matplotlib import pyplot as plt\n\nresults = predictr.evals_result()\n\nplt.figure(figsize=(8,6))\nplt.plot(results[\"validation_0\"][\"logloss\"], label=\"Training loss\")\nplt.plot(results[\"validation_1\"][\"logloss\"], label=\"Validation loss\")\nplt.axvline(predictr.best_iteration, color=\"gray\", label=\"Optimal tree number\")\nplt.xlabel(\"Number of trees\")\nplt.ylabel(\"Loss\")\nplt.legend();\n\n\n\n\n\n튜닝 + early stopping\nhttps://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline\n\npredictr = XGBClassifier(n_estimators = 1000,\n                         random_state = 42, \n                         scale_pos_weight = 1,\n                         eval_metric='logloss', \n                         early_stopping_rounds = 50)\n\npipe = Pipeline([('ct', ct), (\"XGBoost\", predictr)]);pipe\n\nPipeline(steps=[('ct',\n                 ColumnTransformer(n_jobs=-1, remainder='passthrough',\n                                   transformers=[('scaler', StandardScaler(),\n                                                  Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                                 ('onehot',\n                                                  OneHotEncoder(drop='first',\n                                                                handle_unknown='ignore',\n                                                                sparse_output=False),\n                                                  Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smok...\n                               feature_types=None, gamma=None, gpu_id=None,\n                               grow_policy=None, importance_type=None,\n                               interaction_constraints=None, learning_rate=None,\n                               max_bin=None, max_cat_threshold=None,\n                               max_cat_to_onehot=None, max_delta_step=None,\n                               max_depth=None, max_leaves=None,\n                               min_child_weight=None, missing=nan,\n                               monotone_constraints=None, n_estimators=1000,\n                               n_jobs=None, num_parallel_tree=None,\n                               predictor=None, random_state=42, ...))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('ct',\n                 ColumnTransformer(n_jobs=-1, remainder='passthrough',\n                                   transformers=[('scaler', StandardScaler(),\n                                                  Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                                 ('onehot',\n                                                  OneHotEncoder(drop='first',\n                                                                handle_unknown='ignore',\n                                                                sparse_output=False),\n                                                  Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smok...\n                               feature_types=None, gamma=None, gpu_id=None,\n                               grow_policy=None, importance_type=None,\n                               interaction_constraints=None, learning_rate=None,\n                               max_bin=None, max_cat_threshold=None,\n                               max_cat_to_onehot=None, max_delta_step=None,\n                               max_depth=None, max_leaves=None,\n                               min_child_weight=None, missing=nan,\n                               monotone_constraints=None, n_estimators=1000,\n                               n_jobs=None, num_parallel_tree=None,\n                               predictor=None, random_state=42, ...))])ct: ColumnTransformerColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])scalerIndex(['age', 'avg_glucose_level', 'bmi'], dtype='object')StandardScalerStandardScaler()onehotIndex(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object')OneHotEncoderOneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)remainder[]passthroughpassthroughXGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=50,\n              enable_categorical=False, eval_metric='logloss',\n              feature_types=None, gamma=None, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, n_estimators=1000, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=42, ...)\n\n\n\nparam_dist = {\n    'XGBoost__learning_rate': scipy.stats.uniform(0.01, 1.99),\n    'XGBoost__n_estimators': scipy.stats.randint(50, 150),\n    'XGBoost__max_depth': scipy.stats.randint(4, 8),\n    'XGBoost__colsample_bytree': scipy.stats.uniform(0.4, 0.4),\n    'XGBoost__gamma' : scipy.stats.randint(3, 6),\n    'XGBoost__min_child_weight' : scipy.stats.randint(8, 16),\n    'XGBoost__subsample' : scipy.stats.uniform(0.4, 0.4),\n}\n\n\nstart_time = time.time()\n\npredictr_rs = RandomizedSearchCV(pipe, \n                                 param_dist,\n                                 n_iter = 10, # default\n                                 cv=5, \n                                 scoring='accuracy',\n                                 random_state=42, \n                                 refit = True,\n                                 n_jobs = -1)\n\nfit_params = {\"XGBoost__eval_set\" : [(ct.fit_transform(X_train), y_train)]}\nverbose = {\"XGBoost__verbose\": False}\n\npredictr_rs.fit(X_train, y_train, **fit_params, **verbose)\npipe.set_params(**{key: round(value, 2) for key, value in predictr_rs.best_params_.items()}).fit(X_train, y_train, **fit_params, **verbose)\n\nend_time = time.time()\nprint(\"코드 실행 시간: {:.1f} 초\".format(end_time - start_time))\n\n코드 실행 시간: 6.1 초\n\n\n\nstart_time = time.time()\n\nscores = cross_validate(estimator = pipe,\n                        X=X_train,\n                        y=y_train,\n                        fit_params = fit_params,\n                        scoring = ['accuracy', 'roc_auc'],\n                        cv=5,\n                        n_jobs = -1)\n\naccuracy_score_rs = np.mean(scores['test_accuracy']).round(4);accuracy_score\nroc_auc_score_rs = np.mean(scores['test_roc_auc']).round(4);roc_auc_score\n\nend_time = time.time()\nprint(\"코드 실행 시간: {:.1f} 초\".format(end_time - start_time))\n\n[0] validation_0-logloss:0.44247\n코드 실행 시간: 0.9 초\n\n\n\nprint(\"Best Parameters: \", predictr_rs.best_params_)\nprint(\"Best Parameters after round: \", pipe.named_steps[\"XGBoost\"].set_params)\nprint(\"Best Score in CV: \", predictr_rs.best_score_)\nprint(\"mean Score after round:\", accuracy_score_rs)\nprint(\"Best test Score: \", pipe.named_steps[\"XGBoost\"].score(ct.transform(X_test), y_test))\n\nBest Parameters:  {'XGBoost__colsample_bytree': 0.5498160475389451, 'XGBoost__gamma': 3, 'XGBoost__learning_rate': 0.37503523183366594, 'XGBoost__max_depth': 7, 'XGBoost__min_child_weight': 12, 'XGBoost__n_estimators': 70, 'XGBoost__subsample': 0.46240745617697465}\nBest Parameters after round:  &lt;bound method XGBModel.set_params of XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.55, early_stopping_rounds=50,\n              enable_categorical=False, eval_metric='logloss',\n              feature_types=None, gamma=3, gpu_id=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=0.38, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=7,\n              max_leaves=None, min_child_weight=12, missing=nan,\n              monotone_constraints=None, n_estimators=70, n_jobs=None,\n              num_parallel_tree=None, predictor=None, random_state=42, ...)&gt;\nBest Score in CV:  0.9515533303606432\nmean Score after round: 0.9511\nBest test Score:  0.9510763209393346"
  },
  {
    "objectID": "posts/13wk.html#a.-전처리",
    "href": "posts/13wk.html#a.-전처리",
    "title": "13wk: 예측 - RandomForest",
    "section": "A. 전처리",
    "text": "A. 전처리\n\nX = df.drop([\"stroke\"], axis=1)\ny = LabelEncoder().fit_transform(df['stroke'])\n\nX_num = X.select_dtypes(include = 'number')\nX_cat = X.select_dtypes(exclude = 'number')\n\nX[X_num.columns] = SimpleImputer(strategy=\"mean\").fit_transform(X_num)\nX[X_cat.columns] = SimpleImputer(strategy=\"most_frequent\").fit_transform(X_cat)\n\n\nscaler = StandardScaler()\n\nonehot = OneHotEncoder(drop = 'first', handle_unknown='ignore', sparse_output=False)\n\nct = ColumnTransformer([('scaler', scaler, X_num.columns),\n                        ('onehot', onehot, X_cat.columns)], \n                       remainder='passthrough', n_jobs=-1)\n\nct\n\nColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformerColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])scalerIndex(['age', 'avg_glucose_level', 'bmi'], dtype='object')StandardScalerStandardScaler()onehotIndex(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object')OneHotEncoderOneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)remainderpassthroughpassthrough"
  },
  {
    "objectID": "posts/14wk.html#a.-전처리",
    "href": "posts/14wk.html#a.-전처리",
    "title": "14wk: 예측 - XGBoost",
    "section": "A. 전처리",
    "text": "A. 전처리\n\nX = df.drop([\"stroke\"], axis=1)\ny = LabelEncoder().fit_transform(df['stroke'])\n\nX_num = X.select_dtypes(include = 'number')\nX_cat = X.select_dtypes(exclude = 'number')\n\nX[X_num.columns] = SimpleImputer(strategy=\"mean\").fit_transform(X_num)\nX[X_cat.columns] = SimpleImputer(strategy=\"most_frequent\").fit_transform(X_cat)\n\n\nscaler = StandardScaler()\n\nonehot = OneHotEncoder(drop = 'first', handle_unknown='ignore', sparse_output=False)\n\nct = ColumnTransformer([('scaler', scaler, X_num.columns),\n                        ('onehot', onehot, X_cat.columns)], \n                       remainder='passthrough', n_jobs=-1)\n\nct\n\nColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformerColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])scalerIndex(['age', 'avg_glucose_level', 'bmi'], dtype='object')StandardScalerStandardScaler()onehotIndex(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object')OneHotEncoderOneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)remainderpassthroughpassthrough"
  },
  {
    "objectID": "posts/14wk.html#파라미터",
    "href": "posts/14wk.html#파라미터",
    "title": "14wk: 예측 - XGBoost",
    "section": "파라미터",
    "text": "파라미터\n\nlearning_rate\n\n학습 단계별로 가중치를 얼마나 사용할 것인지 결정\n값이 작을수록 다음 단계의 결과물 적게 반영\n기본값 = 0.1, 일반적으로 0.01 ~ 0.2 결정\n범위 [0, 1]\n\nn_estimators\n\n생성할 weak learner의 수\n기본값 100, 일반적으로결정\n\nmin_child_weight\n\n관측치에 대한 인스턴스 가중치합의 최소\n값이 클수록 과적합 방지\n기본값 1\n[0, ∞]\n\nmin_split_loss (= gamma)\n\n리프노드의 추가 분할을 결정할 최소 손실 감소값\n기본값 = 0\n값이 클수록 과적합 방지\n범위: [0, ∞]\n\nmax_depth\n\n트리 깊이\n기본값 6, 일반적으로 3 ~ 10 사용.\n값이 작을수록 과적합 방지\n\nsubsample\n\n훈련 데이터의 일부를 무작위로 샘플링하는 비율\n기본값 1, 일반적으로 0.5 ~ 1 사이 값 사용\n각 부스팅 반복(iteration)마다 랜덤 샘플링 수행.\n값이 작을수록 과적합 방지\n범위: (0, 1]\n\ncolsample_bytree\n\n각 트리마다 피쳐 샘플링 비율\n기본값 1, 일반적으로 0.5 ~ 1 사이 값 사용\n\nscale_pos_weight\n\n불균형 데이터셋의 균형 유지를 위해 사용\n기본값 1\n\nreg_lambda\n\nL2규제(릿지) 가중치\n클수록 과적합 감소\n기본값 1\n\nreg_alpah\n\nL1규제(라쏘) 가중치\n클수록 과적합 감소\n기본값 0\n\nobjective\n\nreg:linear : 회귀 (default)\nbinary:logistict : 이진분류\nmulti:softmax : 다중분류, 클래스 반환\nmulti:softprob : 다중분류, 확률 반환\n\neval_metric\n\n목적함수에 따라 디폴트 값이 다름 (회귀 분석 : rmse, 클래스 분류: error)\nrmse\nmae\nlogloss\n\n예측 확률과 실제 라벨 사이의 차이\n\nerror\n\n예측라벨과 실제 라벨의 다른 비율\n\nmerror\nmlogloss\nauc"
  },
  {
    "objectID": "posts/06wk.html",
    "href": "posts/06wk.html",
    "title": "05wk: 전처리: 데이터 형태 갖추기 - 클래스 불균형",
    "section": "",
    "text": "Comming Soon"
  },
  {
    "objectID": "posts/02wk.html",
    "href": "posts/02wk.html",
    "title": "02wk: 전처리: 데이터 형태 갖추기 - 결측치",
    "section": "",
    "text": "데이터 출처\n\nhttps://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset/data\n\n\n\nimport pandas as pd\n\n\ndata = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\ndata = data.drop(['id'], axis=1)\ndata = data[data['gender'] != 'Other']\ndata.head()\n\ndf = data.copy()\ndf.loc[:, [\"hypertension\", \"heart_disease\", \"stroke\"]] = data.loc[:, [\"hypertension\", \"heart_disease\", \"stroke\"]].applymap(lambda x: \"Yes\" if x == 1 else \"No\")\ndf.head()\n\n\n\n\n\n\n\n\ngender\nage\nhypertension\nheart_disease\never_married\nwork_type\nResidence_type\navg_glucose_level\nbmi\nsmoking_status\nstroke\n\n\n\n\n0\nMale\n67.0\nNo\nYes\nYes\nPrivate\nUrban\n228.69\n36.6\nformerly smoked\nYes\n\n\n1\nFemale\n61.0\nNo\nNo\nYes\nSelf-employed\nRural\n202.21\nNaN\nnever smoked\nYes\n\n\n2\nMale\n80.0\nNo\nYes\nYes\nPrivate\nRural\n105.92\n32.5\nnever smoked\nYes\n\n\n3\nFemale\n49.0\nNo\nNo\nYes\nPrivate\nUrban\n171.23\n34.4\nsmokes\nYes\n\n\n4\nFemale\n79.0\nYes\nNo\nYes\nSelf-employed\nRural\n174.12\n24.0\nnever smoked\nYes"
  },
  {
    "objectID": "posts/02wk.html#a.-결측치-확인",
    "href": "posts/02wk.html#a.-결측치-확인",
    "title": "02wk: 전처리: 데이터 형태 갖추기 - 결측치",
    "section": "A. 결측치 확인",
    "text": "A. 결측치 확인\n\n[col for col in df.columns if df[col].isnull().sum() &gt; 0]\n\n['bmi']\n\n\n\ndf.isna().sum()\n\nid                     0\ngender                 0\nage                    0\nhypertension           0\nheart_disease          0\never_married           0\nwork_type              0\nResidence_type         0\navg_glucose_level      0\nbmi                  201\nsmoking_status         0\nstroke                 0\ndtype: int64"
  },
  {
    "objectID": "posts/02wk.html#b.-결측치-대체",
    "href": "posts/02wk.html#b.-결측치-대체",
    "title": "02wk: 전처리: 데이터 형태 갖추기 - 결측치",
    "section": "B. 결측치 대체",
    "text": "B. 결측치 대체\n\n1) SimpleImputer()\n\n한 특성(열)의 통계 값을 이용하여 결측치 채움.\n\n- 연속형 변수 - strategy = \"mean\", \"median\", \"most_frequent\", \"constant, fill_value\"\n- 범주형 변수 - strategy = \"most_frequent\", \"constant, fill_value\"\n\nfrom sklearn.impute import SimpleImputer\n\n\ndef imputed_missing(df):\n    df_imputed = df.copy()\n    df_num = df.select_dtypes(include=\"number\")\n    df_cat = df.select_dtypes(exclude=\"number\")\n    df_imputed[df_num.columns] = SimpleImputer().fit_transform(df_num) \n    df_imputed[df_cat.columns] = SimpleImputer(strategy='most_frequent').fit_transform(df_cat) \n    return df_imputed\n\n\nimputed_missing(df)\n\n\n\n\n\n\n\n\nid\ngender\nage\nhypertension\nheart_disease\never_married\nwork_type\nResidence_type\navg_glucose_level\nbmi\nsmoking_status\nstroke\n\n\n\n\n0\n9046.0\nMale\n67.0\nNo\nYes\nYes\nPrivate\nUrban\n228.69\n36.600000\nformerly smoked\nYes\n\n\n1\n51676.0\nFemale\n61.0\nNo\nNo\nYes\nSelf-employed\nRural\n202.21\n28.893237\nnever smoked\nYes\n\n\n2\n31112.0\nMale\n80.0\nNo\nYes\nYes\nPrivate\nRural\n105.92\n32.500000\nnever smoked\nYes\n\n\n3\n60182.0\nFemale\n49.0\nNo\nNo\nYes\nPrivate\nUrban\n171.23\n34.400000\nsmokes\nYes\n\n\n4\n1665.0\nFemale\n79.0\nYes\nNo\nYes\nSelf-employed\nRural\n174.12\n24.000000\nnever smoked\nYes\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5105\n18234.0\nFemale\n80.0\nYes\nNo\nYes\nPrivate\nUrban\n83.75\n28.893237\nnever smoked\nNo\n\n\n5106\n44873.0\nFemale\n81.0\nNo\nNo\nYes\nSelf-employed\nUrban\n125.20\n40.000000\nnever smoked\nNo\n\n\n5107\n19723.0\nFemale\n35.0\nNo\nNo\nYes\nSelf-employed\nRural\n82.99\n30.600000\nnever smoked\nNo\n\n\n5108\n37544.0\nMale\n51.0\nNo\nNo\nYes\nPrivate\nRural\n166.29\n25.600000\nformerly smoked\nNo\n\n\n5109\n44679.0\nFemale\n44.0\nNo\nNo\nYes\nGovt_job\nUrban\n85.28\n26.200000\nUnknown\nNo\n\n\n\n\n5110 rows × 12 columns\n\n\n\n\n\n2) .fillna()\n\ncol = [col for col in df.columns if df[col].isnull().sum() &gt; 0]\n\ndf_a = df.copy()\ndf_a[col] = df_a[col].fillna(df[col].mean())\ndf_a.head()\n\n\n\n\n\n\n\n\nid\ngender\nage\nhypertension\nheart_disease\never_married\nwork_type\nResidence_type\navg_glucose_level\nbmi\nsmoking_status\nstroke\n\n\n\n\n0\n9046\nMale\n67.0\nNo\nYes\nYes\nPrivate\nUrban\n228.69\n36.600000\nformerly smoked\nYes\n\n\n1\n51676\nFemale\n61.0\nNo\nNo\nYes\nSelf-employed\nRural\n202.21\n28.893237\nnever smoked\nYes\n\n\n2\n31112\nMale\n80.0\nNo\nYes\nYes\nPrivate\nRural\n105.92\n32.500000\nnever smoked\nYes\n\n\n3\n60182\nFemale\n49.0\nNo\nNo\nYes\nPrivate\nUrban\n171.23\n34.400000\nsmokes\nYes\n\n\n4\n1665\nFemale\n79.0\nYes\nNo\nYes\nSelf-employed\nRural\n174.12\n24.000000\nnever smoked\nYes\n\n\n\n\n\n\n\n\n\n3) IterativeImputer()\n\n다른 특성(열)을 사용하여 결측치 예측.\ninitial_strategy 매개 변수: mean, median, most_frequent, constant\n\n지정된 방식으로 결측치 초기화.\n결측치가 있는 한 특성을 다른 특성들을 사용한 모델을 훈련하여 예측\n\nimputation_order 매개 변수:\n\nascending: 결측치 값이 가장 적은 특성부터\ndescending: 결측치 값이 가장 큰 특성부터\nroman: 왼쪽에서 오른쪽으로\narabic: 오른쪽에서 왼쪽으로\nrandom: 무작위\nmax_iter 매개 변수: 지정된 횟수에 도달 할 때 종료. 기본값: 10\ntol: 각 반복 단계에서 이전 단계와 절댓값 차이 중 가장 큰 값이 누락된 값을 제외하고 가장 큰 절댓값과 지정된 값을 곱한 것보다 작을 경우 종료. 기본값: 0.001\nn_nearest_features: 예측에 사용할 특성 계수. 상관 계수가 높은 특성을 우선하여 랜덤하게 선택 . 기본값: None\nestimator: 예측에 사용되는 모델. 기본값: BayesianRidge\n\nenable_iterative_imputer 클래스: IterativeImputer 클래스가 실험적이기 때문에 이 클래스를 먼저 import 해야함\n\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n\nitrimr = IterativeImputer()\nitrimr.fit_transform(df.select_dtypes(include=\"number\"))\n\narray([[9.04600000e+03, 6.70000000e+01, 2.28690000e+02, 3.66000000e+01],\n       [5.16760000e+04, 6.10000000e+01, 2.02210000e+02, 3.25949405e+01],\n       [3.11120000e+04, 8.00000000e+01, 1.05920000e+02, 3.25000000e+01],\n       ...,\n       [1.97230000e+04, 3.50000000e+01, 8.29900000e+01, 3.06000000e+01],\n       [3.75440000e+04, 5.10000000e+01, 1.66290000e+02, 2.56000000e+01],\n       [4.46790000e+04, 4.40000000e+01, 8.52800000e+01, 2.62000000e+01]])\n\n\n\n\n4) KNNImputer()\n\nk-최근접 이웃 방법을 사용하여 결측치를 채움.\nn_neighbors 매개변수: 최근접 이웃 개수. 기본값 5\n\n샘플개수가 n_neighbors가 작으면 SimpleImputer()\n\n\n\nfrom sklearn.impute import KNNImputer\n\n\nkimr = KNNImputer(n_neighbors = 5) # 샘플 갯수가 n_neighbors보다 작으면 mean 계산\nkimr.fit_transform(df.select_dtypes(include=\"number\"))\n\narray([[9.0460e+03, 6.7000e+01, 2.2869e+02, 3.6600e+01],\n       [5.1676e+04, 6.1000e+01, 2.0221e+02, 2.8080e+01],\n       [3.1112e+04, 8.0000e+01, 1.0592e+02, 3.2500e+01],\n       ...,\n       [1.9723e+04, 3.5000e+01, 8.2990e+01, 3.0600e+01],\n       [3.7544e+04, 5.1000e+01, 1.6629e+02, 2.5600e+01],\n       [4.4679e+04, 4.4000e+01, 8.5280e+01, 2.6200e+01]])"
  },
  {
    "objectID": "posts/11wk.html",
    "href": "posts/11wk.html",
    "title": "11wk: 예측 모델 훈련과 선택 - 평가 지표",
    "section": "",
    "text": "Comming Soon"
  },
  {
    "objectID": "posts/05wk.html",
    "href": "posts/05wk.html",
    "title": "05wk: 전처리: 데이터 형태 갖추기 - 차원 축소",
    "section": "",
    "text": "Comming Soon"
  },
  {
    "objectID": "posts/12wk..html#a.-전처리",
    "href": "posts/12wk..html#a.-전처리",
    "title": "12wk: 예측 - LogisticRegression",
    "section": "A. 전처리",
    "text": "A. 전처리\n\nX = df.drop([\"stroke\"], axis=1)\ny = LabelEncoder().fit_transform(df['stroke'])\n\nX_num = X.select_dtypes(include = 'number')\nX_cat = X.select_dtypes(exclude = 'number')\n\nX[X_num.columns] = SimpleImputer(strategy=\"mean\").fit_transform(X_num)\nX[X_cat.columns] = SimpleImputer(strategy=\"most_frequent\").fit_transform(X_cat)\n\n\nscaler = StandardScaler()\n\nonehot = OneHotEncoder(drop = 'first', handle_unknown='ignore', sparse_output=False)\n\nct = ColumnTransformer([('scaler', scaler, X_num.columns),\n                        ('onehot', onehot, X_cat.columns)], \n                       remainder='passthrough', n_jobs=-1)\n\nct\n\nColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformerColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])scalerIndex(['age', 'avg_glucose_level', 'bmi'], dtype='object')StandardScalerStandardScaler()onehotIndex(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object')OneHotEncoderOneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)remainderpassthroughpassthrough"
  },
  {
    "objectID": "posts/01wk.html",
    "href": "posts/01wk.html",
    "title": "01wk: 머신 러닝 교과서 - 파이토치편 소개",
    "section": "",
    "text": "데이터를 지식으로 바꾸는 지능적인 시스템 구축\n머신 러닝의 세 가지 종류\n\n지도 학습으로 미래 예측\n\n분류: 클래스 레이블 예측\n회귀: 연속적인 출력 값 예측\n\n강화 학습으로 반응형 문제 해결\n비지도 학습으로 숨겨진 구조 발견\n\n군집: 서브그룹 찾기\n차원 축소: 데이터 압축\n\n\n머신 러닝 시스템 구축 로드맵\n\n전처리: 데이터 형태 갖추기\n예측 모델 훈련과 선택\n예측\n\n\n\nfrom IPython.display import Image"
  },
  {
    "objectID": "posts/01wk.html#지도-학습으로-미래-예측",
    "href": "posts/01wk.html#지도-학습으로-미래-예측",
    "title": "01wk: 머신 러닝 교과서 - 파이토치편 소개",
    "section": "지도 학습으로 미래 예측",
    "text": "지도 학습으로 미래 예측\n\n데이터 입력과 레이블 사이의 관계를 모델링하는 과정\n\n\nImage(url='https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch01/figures/01_02.png', width=500)\n\n\n\n\n\n분류: 클래스 레이블 예측\n\nImage(url='https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch01/figures/01_03.png', width=300)\n\n\n\n\n\n\n회귀: 연속적인 출력 값 예측\n\nImage(url='https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch01/figures/01_04.png', width=300)"
  },
  {
    "objectID": "posts/01wk.html#강화-학습으로-반응형-문제-해결",
    "href": "posts/01wk.html#강화-학습으로-반응형-문제-해결",
    "title": "01wk: 머신 러닝 교과서 - 파이토치편 소개",
    "section": "강화 학습으로 반응형 문제 해결",
    "text": "강화 학습으로 반응형 문제 해결\n\n환경과 상호 작용하여 에이전트 성능 향상 목적\n\n\nImage(url='https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch01/figures/01_05.png', width=300)\n\n\n\n\n\n예) 체스 게임\n\n체스판의 상태(환경)에 따라 기물의 이동 결정.\n기물의 이동의 결과는 각기 다른 환경 상태. (어떤 움직임은 승리로 이어질 가능성이 높고 어떤 움직임은 패배로 이어질 가능성이 높음).\n보상은 게임을 종료했을 때 승리하거나 패배로 정의.\n기물 이동(행동)을 수행하여 얻어진 피드백을 통해 전체 보상을 최대화."
  },
  {
    "objectID": "posts/01wk.html#비지도-학습으로-숨겨진-구조-발견",
    "href": "posts/01wk.html#비지도-학습으로-숨겨진-구조-발견",
    "title": "01wk: 머신 러닝 교과서 - 파이토치편 소개",
    "section": "비지도 학습으로 숨겨진 구조 발견",
    "text": "비지도 학습으로 숨겨진 구조 발견\n\n레이블되지 않거나 구조를 알 수 없는 데이터에서 정보 추출.\n\n\n군집: 서브그룹 찾기\n\nImage(url='https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch01/figures/01_06.png', width=300)\n\n\n\n\n\n관심사를 기반으로 고객을 그룹으로 나누어 각각에 맞는 마케팅\n\n\n\n차원 축소: 데이터 압축\n\nImage(url='https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch01/figures/01_07.png', width=500)\n\n\n\n\n\n잡음 데이터를 제거하기 위해 전처리 단계에서 적용."
  },
  {
    "objectID": "posts/01wk.html#전처리-데이터-형태-갖추기",
    "href": "posts/01wk.html#전처리-데이터-형태-갖추기",
    "title": "01wk: 머신 러닝 교과서 - 파이토치편 소개",
    "section": "전처리: 데이터 형태 갖추기",
    "text": "전처리: 데이터 형태 갖추기\n\n결측치 대체\n훈련 셋 테스트 셋 나누기\n\n새로운 데이터에서도 잘 일반화되는지 확인\n훈련 셋으로 머신 러닝 모델 훈련하고 최적화\n테스트 셋으로 최종 모델 평가\n\n표준화 및 정규화\n\n스케일에 민감한 머신 러닝 알고리즘 (선형모형, SVM, XGBoost 등)은 같은 단위로 맞춰주어야함.\n트리 기반 모형은 스케일에 민감하지 않음.\n전체 데이터 표준화보다 훈련 셋으로 표준화 진행하고 그 기준에 맞게 테스트 셋도 표준화.\n\n차원 축소\n\n특성 간 상관관계가 높거나 관련 없는 특성 (또는 잡음)이 매우 많을 경우 사용"
  },
  {
    "objectID": "posts/01wk.html#예측-모델-훈련과-선택",
    "href": "posts/01wk.html#예측-모델-훈련과-선택",
    "title": "01wk: 머신 러닝 교과서 - 파이토치편 소개",
    "section": "예측 모델 훈련과 선택",
    "text": "예측 모델 훈련과 선택\n\n교차 검증 셋으로 훈련 모델 평가 및 과적합 해소\n하이퍼파라미터 튜닝\n\n그리드 서치\n랜덤 서치\n베이지안 최적화\n\n위 두가지 방법으로 최적의 머신 러닝 모델 평가 지표 점수 비교"
  },
  {
    "objectID": "posts/01wk.html#예측",
    "href": "posts/01wk.html#예측",
    "title": "01wk: 머신 러닝 교과서 - 파이토치편 소개",
    "section": "예측",
    "text": "예측\n\n훈련 셋에서 최적의 모델을 선택한 후에 테스트 셋 예측"
  },
  {
    "objectID": "posts/16wk.html#a.-전처리",
    "href": "posts/16wk.html#a.-전처리",
    "title": "16wk: 예측 - MLPClassifier",
    "section": "A. 전처리",
    "text": "A. 전처리\n\nX = df.drop([\"stroke\"], axis=1)\ny = LabelEncoder().fit_transform(df['stroke'])\n\nX_num = X.select_dtypes(include = 'number')\nX_cat = X.select_dtypes(exclude = 'number')\n\nX[X_num.columns] = SimpleImputer(strategy=\"mean\").fit_transform(X_num)\nX[X_cat.columns] = SimpleImputer(strategy=\"most_frequent\").fit_transform(X_cat)\n\n\nscaler = StandardScaler()\n\nonehot = OneHotEncoder(drop = 'first', handle_unknown='ignore', sparse_output=False)\n\nct = ColumnTransformer([('scaler', scaler, X_num.columns),\n                        ('onehot', onehot, X_cat.columns)], \n                       remainder='passthrough', n_jobs=-1)\n\nct\n\nColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformerColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])scalerIndex(['age', 'avg_glucose_level', 'bmi'], dtype='object')StandardScalerStandardScaler()onehotIndex(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object')OneHotEncoderOneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)remainderpassthroughpassthrough"
  },
  {
    "objectID": "posts/16wk.html#파라미터",
    "href": "posts/16wk.html#파라미터",
    "title": "16wk: 예측 - MLPClassifier",
    "section": "파라미터",
    "text": "파라미터\n\nhidden_layer_sizes\n\n은닉층 뉴런 개수.\narray-like of shape(n_layers - 2,)\n기본값 (100,)\n\nactivation\n\n은닉층 활성 함수\n기본값 relu {identity, logistic, tanh, relu}\n\nsolver`\n\n경사 하강법의 종류\n기본값adam {lbfgs, sgd, adam}\n\nlbfgs : an optimizer in the family of quasi-Newton methods. (small dataset)\nsgd : stochastic gradient descent.\nadam : stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba (large datasets)\ncomparison between Adam optimizer and SGD\nhttps://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_training_curves.html#sphx-glr-auto-examples-neural-networks-plot-mlp-training-curves-py\n\n\nalpha\n\nL2 규제의 크기\n기본값 0.0001\n값이 클수록 과적합 방지\nhttps://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_alpha.html#sphx-glr-auto-examples-neural-networks-plot-mlp-alpha-py\n\nbatch_size\n\n확률적 최적화 미니배치 사이즈\n기본값 auto = min(200, n_samples)\n\nlearning_rate\n\n가중치 업데이트 학습률 스케줄.\nsolver가 sgd일 때만 사용 가능\n기본값 constant {constant, invscaling, adaptive}\n\nconstant : ’learning_rate_init’으로 주어진 일정한 학습률.\ninvscaling : ’power_t’의 역 스케일링 지수 사용. 각 시간 단계 t에서 점진적으로 학습률 감소. effective_learning_rate = learning_rate_init / pow(t, power_t)\nadaptive : 학습 손실이 계속 감소하는 한 학습률을 learning_rate_init로 일정하게 유지.\n두 연속 에포크 동안 학습 손실이 tol만큼 감소하지 않거나, early_stopping이 켜져 있을 때 검증 점수가 tol만큼 증가하지 않으면, 현재 학습률이 5로 나누어집니다.\n\n\nlearning_rate_init\n\n초기 학습률. 가중치 업데이트 시 스텝 크기를 조절.\nsolver가 sgd 또는 adam일 때만 사용.\n기본값 0.001\n\npower_\n\n역 스케일링 학습률의 지수.\nlearning_rate가 invscaling으로 설정되었을 때 유효 학습률 업데이트.\nsolver가 sgd일 때만 사용.\n기본값 0.5\n\nmax_iter\n\n최대 반복 횟수.\nsolver가 이 횟수만큼 반복하거나 tol에 의해 수렴이 결정될 때까지 반복.\n확률적 slover(sgd, adam)의 경우, 경사 하강 단계 수가 아닌 에포크 수(각 데이터 포인트가 사용되는 횟수, 전체 데이터셋을 한 번 학습하는 과정) 결정\n기본값 200\n\ntol\n\n최적화를 위한 허용 오차.\n손실이나 점수가 n_iter_no_change 연속 반복 동안 tol만큼 개선되지 않으면, learning_rate가 adaptive로 설정되지 않는 한 수렴에 도달한 것으로 간주하고 학습 중지.\n기본값 0.0001\n\nn_iter_no_change\n\ntol 개선을 충족하지 못하는 최대 에포크 수.\nsolver가 sgd 또는 adam일 때만 사용.\n기본값 10\n\nshuffle\n\n각 반복마다 샘플 셔플.\nsolver가 sgd 또는 adam일 때만 사용.\n\nearly_stopping\n\n검증 점수가 향상되지 않을 때 학습 중지\n만약 True로 설정된 경우, 학습 데이터의 10%를 자동으로 검증 데이터로 분리하고, 검증 점수가 n_iter_no_chang 연속 에포크 동안 tol만큼 향상되지 않으면 학습을 중지.\n분할은 계층화(stratified) (multilabel 설정에서는 제외됨).\nFalse로 설정된 경우, 학습 손실이 tol이 n_iter_no_change 동안 더 이상 개선되지 않을 때 학습 중지.\nsolver가 sgd 또는 adam일 때만 사용.\n\nvalidation_fraction\n\nearly_stopping = True 설정 시, 검증 세트 비율.\n기본값 0.1 (0, 1)"
  },
  {
    "objectID": "posts/09wk.html",
    "href": "posts/09wk.html",
    "title": "09wk: 예측 모델 훈련과 선택 - 파라미터 튜닝",
    "section": "",
    "text": "데이터 출처\n\nhttps://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset/data\n\n\n\nimport pandas as pd\nimport numpy as np\nimport time\n\n\ndata = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\ndata = data.drop(['id'], axis=1)\ndata = data[data['gender'] != 'Other']\ndata.head()\n\ndf = data.copy()\ndf.loc[:, [\"hypertension\", \"heart_disease\", \"stroke\"]] = data.loc[:, [\"hypertension\", \"heart_disease\", \"stroke\"]].applymap(lambda x: \"Yes\" if x == 1 else \"No\")\ndf.head()\n\n\n\n\n\n\n\n\ngender\nage\nhypertension\nheart_disease\never_married\nwork_type\nResidence_type\navg_glucose_level\nbmi\nsmoking_status\nstroke\n\n\n\n\n0\nMale\n67.0\nNo\nYes\nYes\nPrivate\nUrban\n228.69\n36.6\nformerly smoked\nYes\n\n\n1\nFemale\n61.0\nNo\nNo\nYes\nSelf-employed\nRural\n202.21\nNaN\nnever smoked\nYes\n\n\n2\nMale\n80.0\nNo\nYes\nYes\nPrivate\nRural\n105.92\n32.5\nnever smoked\nYes\n\n\n3\nFemale\n49.0\nNo\nNo\nYes\nPrivate\nUrban\n171.23\n34.4\nsmokes\nYes\n\n\n4\nFemale\n79.0\nYes\nNo\nYes\nSelf-employed\nRural\n174.12\n24.0\nnever smoked\nYes\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop([\"stroke\"], axis=1)\ny = LabelEncoder().fit_transform(df['stroke'])\n\n\nfrom sklearn.impute import SimpleImputer\n\nX_num = X.select_dtypes(include = 'number')\nX_cat = X.select_dtypes(exclude = 'number')\n\nX[X_num.columns] = SimpleImputer(strategy=\"mean\").fit_transform(X_num)\nX[X_cat.columns] = SimpleImputer(strategy=\"most_frequent\").fit_transform(X_cat)\n\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nscaler = StandardScaler()\n\nonehot = OneHotEncoder(drop = 'first', handle_unknown='ignore', sparse_output=False)\n\nct = ColumnTransformer([('scaler', scaler, X_num.columns),\n                        ('onehot', onehot, X_cat.columns)], \n                       remainder='passthrough', n_jobs=-1)\n\nct\n\nColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformerColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])scalerIndex(['age', 'avg_glucose_level', 'bmi'], dtype='object')StandardScalerStandardScaler()onehotIndex(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object')OneHotEncoderOneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)remainderpassthroughpassthrough"
  },
  {
    "objectID": "posts/09wk.html#a.-전처리",
    "href": "posts/09wk.html#a.-전처리",
    "title": "09wk: 예측 모델 훈련과 선택 - 파라미터 튜닝",
    "section": "",
    "text": "from sklearn.preprocessing import LabelEncoder\n\nX = df.drop([\"stroke\"], axis=1)\ny = LabelEncoder().fit_transform(df['stroke'])\n\n\nfrom sklearn.impute import SimpleImputer\n\nX_num = X.select_dtypes(include = 'number')\nX_cat = X.select_dtypes(exclude = 'number')\n\nX[X_num.columns] = SimpleImputer(strategy=\"mean\").fit_transform(X_num)\nX[X_cat.columns] = SimpleImputer(strategy=\"most_frequent\").fit_transform(X_cat)\n\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nscaler = StandardScaler()\n\nonehot = OneHotEncoder(drop = 'first', handle_unknown='ignore', sparse_output=False)\n\nct = ColumnTransformer([('scaler', scaler, X_num.columns),\n                        ('onehot', onehot, X_cat.columns)], \n                       remainder='passthrough', n_jobs=-1)\n\nct\n\nColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformerColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])scalerIndex(['age', 'avg_glucose_level', 'bmi'], dtype='object')StandardScalerStandardScaler()onehotIndex(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object')OneHotEncoderOneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)remainderpassthroughpassthrough"
  },
  {
    "objectID": "posts/09wk.html#a.-gridsearchcv",
    "href": "posts/09wk.html#a.-gridsearchcv",
    "title": "09wk: 예측 모델 훈련과 선택 - 파라미터 튜닝",
    "section": "A. GridSearchCV()",
    "text": "A. GridSearchCV()\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, cross_validate, cross_val_predict\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n\npredictr = LogisticRegression(random_state=42, max_iter = 1000)\n\npipe = Pipeline([('ct', ct), (\"model\", predictr)]);pipe\n\nPipeline(steps=[('ct',\n                 ColumnTransformer(n_jobs=-1, remainder='passthrough',\n                                   transformers=[('scaler', StandardScaler(),\n                                                  Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                                 ('onehot',\n                                                  OneHotEncoder(drop='first',\n                                                                handle_unknown='ignore',\n                                                                sparse_output=False),\n                                                  Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])),\n                ('model', LogisticRegression(max_iter=1000, random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('ct',\n                 ColumnTransformer(n_jobs=-1, remainder='passthrough',\n                                   transformers=[('scaler', StandardScaler(),\n                                                  Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                                 ('onehot',\n                                                  OneHotEncoder(drop='first',\n                                                                handle_unknown='ignore',\n                                                                sparse_output=False),\n                                                  Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])),\n                ('model', LogisticRegression(max_iter=1000, random_state=42))])ct: ColumnTransformerColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])scalerIndex(['age', 'avg_glucose_level', 'bmi'], dtype='object')StandardScalerStandardScaler()onehotIndex(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object')OneHotEncoderOneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)remainderpassthroughpassthroughLogisticRegressionLogisticRegression(max_iter=1000, random_state=42)\n\n\n\nparam_grid = {\n    'model__C': np.logspace(-4, 4, 10)\n}\n\n\nfrom sklearn.model_selection import GridSearchCV\n\nstart_time = time.time()\n\npredictr_gs = GridSearchCV(pipe, \n                           param_grid, \n                           cv=5, \n                           scoring='accuracy',\n                           refit = True,\n                           n_jobs = -1)\n\npredictr_gs.fit(X_train, y_train)\n\npipe.set_params(**{key: value for key, value in predictr_gs.best_params_.items()}).fit(X_train, y_train)\n\nend_time = time.time()\nprint(\"코드 실행 시간: {:.1f} 초\".format(end_time - start_time))\n\n코드 실행 시간: 5.0 초\n\n\n\nstart_time = time.time()\n\n# estimator = pipe에 predictr_gs 수행 시 반올림 하기 전 파라미터로 수행. (시간이 더 소요)\nscores = cross_validate(estimator = pipe,\n                        X=X_train,\n                        y=y_train,\n                        scoring = ['accuracy', 'roc_auc'], # 'accuracy', 'precision', 'recall', 'f1', 'roc_auc'\n                        cv=5,\n                        n_jobs = -1)\n\naccuracy_score_gs = np.mean(scores['test_accuracy']).round(4)\n\nend_time = time.time()\nprint(\"코드 실행 시간: {:.1f} 초\".format(end_time - start_time))\n\n코드 실행 시간: 0.8 초\n\n\n\nprint(\"Best Parameters: \", predictr_gs.best_params_)\nprint(\"Best Parameters after round: \", pipe.named_steps[\"model\"].set_params)\nprint(\"Best Score in CV: \", predictr_gs.best_score_)\nprint(\"mean Score after round:\", accuracy_score_gs)\nprint(\"Best test Score: \", pipe.named_steps[\"model\"].score(ct.transform(X_test), y_test))\n\nBest Parameters:  {'model__C': 0.0001}\nBest Parameters after round:  &lt;bound method BaseEstimator.set_params of LogisticRegression(C=0.0001, max_iter=1000, random_state=42)&gt;\nBest Score in CV:  0.9513091308472467\nmean Score after round: 0.9513\nBest test Score:  0.9510763209393346"
  },
  {
    "objectID": "posts/09wk.html#b.-randomizedsearchcv",
    "href": "posts/09wk.html#b.-randomizedsearchcv",
    "title": "09wk: 예측 모델 훈련과 선택 - 파라미터 튜닝",
    "section": "B. RandomizedSearchCV()",
    "text": "B. RandomizedSearchCV()\n\npredictr = LogisticRegression(random_state=42, max_iter = 1000)\n\npipe = Pipeline([('ct', ct), (\"model\", predictr)]);pipe\n\nPipeline(steps=[('ct',\n                 ColumnTransformer(n_jobs=-1, remainder='passthrough',\n                                   transformers=[('scaler', StandardScaler(),\n                                                  Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                                 ('onehot',\n                                                  OneHotEncoder(drop='first',\n                                                                handle_unknown='ignore',\n                                                                sparse_output=False),\n                                                  Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])),\n                ('model', LogisticRegression(max_iter=1000, random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('ct',\n                 ColumnTransformer(n_jobs=-1, remainder='passthrough',\n                                   transformers=[('scaler', StandardScaler(),\n                                                  Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                                 ('onehot',\n                                                  OneHotEncoder(drop='first',\n                                                                handle_unknown='ignore',\n                                                                sparse_output=False),\n                                                  Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])),\n                ('model', LogisticRegression(max_iter=1000, random_state=42))])ct: ColumnTransformerColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])scalerIndex(['age', 'avg_glucose_level', 'bmi'], dtype='object')StandardScalerStandardScaler()onehotIndex(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object')OneHotEncoderOneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)remainder[]passthroughpassthroughLogisticRegressionLogisticRegression(max_iter=1000, random_state=42)\n\n\n\nimport scipy.stats\n\nparam_dist = {\n    'model__C': scipy.stats.loguniform(1e-4, 1e4)\n}\n\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nstart_time = time.time()\n\npredictr_rs = RandomizedSearchCV(pipe, \n                                 param_dist,\n                                 n_iter = 50, # default\n                                 cv=5, \n                                 scoring='accuracy',\n                                 random_state=42, \n                                 refit = True,\n                                 n_jobs = -1)\n\npredictr_rs.fit(X_train, y_train)\npipe.set_params(**{key: round(value, 2) for key, value in predictr_rs.best_params_.items()}).fit(X_train, y_train)\n\nend_time = time.time()\nprint(\"코드 실행 시간: {:.1f} 초\".format(end_time - start_time))\n\n코드 실행 시간: 4.9 초\n\n\n\nstart_time = time.time()\n\nscores = cross_validate(estimator = pipe,\n                        X=X_train,\n                        y=y_train,\n                        scoring = ['accuracy', 'roc_auc'],\n                        cv=5,\n                        n_jobs = -1)\n\naccuracy_score_rs = np.mean(scores['test_accuracy']).round(4)\nroc_auc_score_rs = np.mean(scores['test_roc_auc']).round(4)\n\nend_time = time.time()\nprint(\"코드 실행 시간: {:.1f} 초\".format(end_time - start_time))\n\n코드 실행 시간: 0.2 초\n\n\n\nprint(\"Best Parameters: \", predictr_rs.best_params_)\nprint(\"Best Parameters after round: \", pipe.named_steps[\"model\"].set_params)\nprint(\"Best Score in CV: \", predictr_rs.best_score_)\nprint(\"mean Score after round:\", accuracy_score_rs)\nprint(\"Best test Score: \", pipe.named_steps[\"model\"].score(ct.transform(X_test), y_test))\n\nBest Parameters:  {'model__C': 0.09915644566638401}\nBest Parameters after round:  &lt;bound method BaseEstimator.set_params of LogisticRegression(C=0.1, max_iter=1000, random_state=42)&gt;\nBest Score in CV:  0.9513091308472467\nmean Score after round: 0.9513\nBest test Score:  0.9510763209393346"
  },
  {
    "objectID": "posts/09wk.html#c.-bayessearchcv",
    "href": "posts/09wk.html#c.-bayessearchcv",
    "title": "09wk: 예측 모델 훈련과 선택 - 파라미터 튜닝",
    "section": "C. BayesSearchCV()",
    "text": "C. BayesSearchCV()\n\npredictr = LogisticRegression(random_state=42, max_iter = 1000)\n\npipe = Pipeline([('ct', ct), (\"model\", predictr)]);pipe\n\nPipeline(steps=[('ct',\n                 ColumnTransformer(n_jobs=-1, remainder='passthrough',\n                                   transformers=[('scaler', StandardScaler(),\n                                                  Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                                 ('onehot',\n                                                  OneHotEncoder(drop='first',\n                                                                handle_unknown='ignore',\n                                                                sparse_output=False),\n                                                  Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])),\n                ('model', LogisticRegression(max_iter=1000, random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('ct',\n                 ColumnTransformer(n_jobs=-1, remainder='passthrough',\n                                   transformers=[('scaler', StandardScaler(),\n                                                  Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                                 ('onehot',\n                                                  OneHotEncoder(drop='first',\n                                                                handle_unknown='ignore',\n                                                                sparse_output=False),\n                                                  Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])),\n                ('model', LogisticRegression(max_iter=1000, random_state=42))])ct: ColumnTransformerColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])scalerIndex(['age', 'avg_glucose_level', 'bmi'], dtype='object')StandardScalerStandardScaler()onehotIndex(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object')OneHotEncoderOneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)remainder[]passthroughpassthroughLogisticRegressionLogisticRegression(max_iter=1000, random_state=42)\n\n\n\nsearch_spaces = {\n    'model__C': (0.001, 1000, 'log-uniform') # scipy.stats.loguniform(1e-4, 1e4)\n}\n\n\nfrom skopt import BayesSearchCV\n\nstart_time = time.time()\n\npredictr_bs = BayesSearchCV(pipe, \n                    search_spaces, \n                    n_iter = 50, # defualt\n                    cv=5, \n                    scoring='accuracy', \n                    random_state=42, \n                    n_jobs = -1)\n\npredictr_bs.fit(X_train, y_train)\npipe.set_params(**{key: round(value, 2) for key, value in predictr_bs.best_params_.items()}).fit(X_train, y_train)\n\nend_time = time.time()\nprint(\"코드 실행 시간: {:.1f} 초\".format(end_time - start_time))\n\n코드 실행 시간: 68.4 초\n\n\n\nstart_time = time.time()\n\nscores = cross_validate(estimator = pipe,\n                        X=X_train,\n                        y=y_train,\n                        scoring = ['accuracy', 'roc_auc'],\n                        cv=5,\n                        n_jobs = -1)\n\naccuracy_score_bs = np.mean(scores['test_accuracy']).round(4)\nroc_auc_score_bs = np.mean(scores['test_roc_auc']).round(4)\n\nend_time = time.time()\nprint(\"코드 실행 시간: {:.1f} 초\".format(end_time - start_time))\n\n코드 실행 시간: 0.2 초\n\n\n\nprint(\"Best Parameters: \", predictr_bs.best_params_)\nprint(\"Best Parameters after round: \", pipe.named_steps[\"model\"].set_params)\nprint(\"Best Score in CV: \", predictr_bs.best_score_)\nprint(\"mean Score after round:\", accuracy_score_bs)\nprint(\"Best test Score: \", pipe.named_steps[\"model\"].score(ct.transform(X_test), y_test))\n\nBest Parameters:  OrderedDict([('model__C', 0.28881766539144715)])\nBest Parameters after round:  &lt;bound method BaseEstimator.set_params of LogisticRegression(C=0.29, max_iter=1000, random_state=42)&gt;\nBest Score in CV:  0.9513091308472467\nmean Score after round: 0.9513\nBest test Score:  0.952054794520548"
  },
  {
    "objectID": "posts/03wk.html",
    "href": "posts/03wk.html",
    "title": "03wk: 전처리: 데이터 형태 갖추기 - 범주형 인코딩",
    "section": "",
    "text": "데이터 출처\n\nhttps://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset/data\n\n\n\nimport pandas as pd\n\n\ndata = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\ndata = data.drop(['id'], axis=1)\ndata = data[data['gender'] != 'Other']\ndata.head()\n\n# 범주형 자료에 숫자형으로 있으면 후에 `.select_dtypes(exclude = 'number')` 수행 하지 못함.\ndf = data.copy()\ndf.loc[:, [\"hypertension\", \"heart_disease\", \"stroke\"]] = data.loc[:, [\"hypertension\", \"heart_disease\", \"stroke\"]].applymap(lambda x: \"Yes\" if x == 1 else \"No\")\ndf.head()\n\n\n\n\n\n\n\n\ngender\nage\nhypertension\nheart_disease\never_married\nwork_type\nResidence_type\navg_glucose_level\nbmi\nsmoking_status\nstroke\n\n\n\n\n0\nMale\n67.0\nNo\nYes\nYes\nPrivate\nUrban\n228.69\n36.6\nformerly smoked\nYes\n\n\n1\nFemale\n61.0\nNo\nNo\nYes\nSelf-employed\nRural\n202.21\nNaN\nnever smoked\nYes\n\n\n2\nMale\n80.0\nNo\nYes\nYes\nPrivate\nRural\n105.92\n32.5\nnever smoked\nYes\n\n\n3\nFemale\n49.0\nNo\nNo\nYes\nPrivate\nUrban\n171.23\n34.4\nsmokes\nYes\n\n\n4\nFemale\n79.0\nYes\nNo\nYes\nSelf-employed\nRural\n174.12\n24.0\nnever smoked\nYes"
  },
  {
    "objectID": "posts/03wk.html#a.-클래스-레이블타깃-변수-인코딩",
    "href": "posts/03wk.html#a.-클래스-레이블타깃-변수-인코딩",
    "title": "03wk: 전처리: 데이터 형태 갖추기 - 범주형 인코딩",
    "section": "A. 클래스 레이블(타깃 변수) 인코딩",
    "text": "A. 클래스 레이블(타깃 변수) 인코딩\n\n1) LabelEncoder()\n\nfrom sklearn.preprocessing import LabelEncoder\n\n\ny = LabelEncoder().fit_transform(df['stroke'].values)\ny\n\narray([1, 1, 1, ..., 0, 0, 0])\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\n참고\nLabelEncoder 클래스\n\n타깃 레이블을 인코딩하기 위한 클래스. 입력 데이터로 1차원 배열을 기대함.\n여러개의 특성(열)을 작업하기에는 반복작업을 해야하므로 범주형 데이터를 정수로 인코딩하는 OrdinalEncode와 판다스 데이터프레임의 열마다 다른 변환을 적용하도록 도와주는 ColumnTransformer를 이용\nOrdinalEncode와 ColumnTransformer는 특성 변환에서 설명\n\n\n\n\n\n\n2) numpy 이용\n\nimport numpy as np\n\n\nY_mapping = {label: idx for idx, label in enumerate(np.unique(df['stroke']))}\nY_mapping\n\n{'No': 0, 'Yes': 1}\n\n\n\nnp.array(df['stroke'].map(Y_mapping))\n\narray([1, 1, 1, ..., 0, 0, 0])"
  },
  {
    "objectID": "posts/03wk.html#b.-원-핫-인코딩",
    "href": "posts/03wk.html#b.-원-핫-인코딩",
    "title": "03wk: 전처리: 데이터 형태 갖추기 - 범주형 인코딩",
    "section": "B. 원-핫 인코딩",
    "text": "B. 원-핫 인코딩\n\n1) OneHotEncoder-ColumnTransformer\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n\nX = df.drop([\"stroke\"], axis=1)\n\n\nonehot = OneHotEncoder(drop = 'first', handle_unknown='ignore', sparse_output=False)\n\nct = ColumnTransformer([('onehot', onehot, X.select_dtypes(exclude = \"number\").columns)],\n                        remainder='passthrough', n_jobs=-1)\nct\n\nColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformerColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])onehotIndex(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object')OneHotEncoderOneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)remainderpassthroughpassthrough\n\n\n\nct.fit_transform(X)\n\narray([[  1.  ,   0.  ,   1.  , ...,  67.  , 228.69,  36.6 ],\n       [  0.  ,   0.  ,   0.  , ...,  61.  , 202.21,    nan],\n       [  1.  ,   0.  ,   1.  , ...,  80.  , 105.92,  32.5 ],\n       ...,\n       [  0.  ,   0.  ,   0.  , ...,  35.  ,  82.99,  30.6 ],\n       [  1.  ,   0.  ,   0.  , ...,  51.  , 166.29,  25.6 ],\n       [  0.  ,   0.  ,   0.  , ...,  44.  ,  85.28,  26.2 ]])\n\n\n\ncategories = ct.named_transformers_['onehot'].categories_\ncategories\n\n[array(['Female', 'Male'], dtype=object),\n array(['No', 'Yes'], dtype=object),\n array(['No', 'Yes'], dtype=object),\n array(['No', 'Yes'], dtype=object),\n array(['Govt_job', 'Never_worked', 'Private', 'Self-employed', 'children'],\n       dtype=object),\n array(['Rural', 'Urban'], dtype=object),\n array(['Unknown', 'formerly smoked', 'never smoked', 'smokes'],\n       dtype=object)]\n\n\n\n# 삭제된 범주와 해당 변수명 확인\ndropped_categories = {feature: category[0] for feature, category in zip(X.select_dtypes(exclude = \"number\").columns, categories)}\nprint(\"삭제된 범주와 변수명:\", dropped_categories)\n\n삭제된 범주와 변수명: {'gender': 'Female', 'hypertension': 'No', 'heart_disease': 'No', 'ever_married': 'No', 'work_type': 'Govt_job', 'Residence_type': 'Rural', 'smoking_status': 'Unknown'}\n\n\n\n\n2) pd.get_dummies(, drop_first = True)\n\npd.get_dummies(X, drop_first = True)\n\n\n\n\n\n\n\n\nage\navg_glucose_level\nbmi\ngender_Male\nhypertension_Yes\nheart_disease_Yes\never_married_Yes\nwork_type_Never_worked\nwork_type_Private\nwork_type_Self-employed\nwork_type_children\nResidence_type_Urban\nsmoking_status_formerly smoked\nsmoking_status_never smoked\nsmoking_status_smokes\n\n\n\n\n0\n67.0\n228.69\n36.6\n1\n0\n1\n1\n0\n1\n0\n0\n1\n1\n0\n0\n\n\n1\n61.0\n202.21\nNaN\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n2\n80.0\n105.92\n32.5\n1\n0\n1\n1\n0\n1\n0\n0\n0\n0\n1\n0\n\n\n3\n49.0\n171.23\n34.4\n0\n0\n0\n1\n0\n1\n0\n0\n1\n0\n0\n1\n\n\n4\n79.0\n174.12\n24.0\n0\n1\n0\n1\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5105\n80.0\n83.75\nNaN\n0\n1\n0\n1\n0\n1\n0\n0\n1\n0\n1\n0\n\n\n5106\n81.0\n125.20\n40.0\n0\n0\n0\n1\n0\n0\n1\n0\n1\n0\n1\n0\n\n\n5107\n35.0\n82.99\n30.6\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n5108\n51.0\n166.29\n25.6\n1\n0\n0\n1\n0\n1\n0\n0\n0\n1\n0\n0\n\n\n5109\n44.0\n85.28\n26.2\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n\n\n5109 rows × 15 columns"
  },
  {
    "objectID": "posts/07wk.html",
    "href": "posts/07wk.html",
    "title": "07wk: 전처리: 데이터 형태 갖추기 - 전처리 총 정리",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split"
  },
  {
    "objectID": "posts/07wk.html#결측치-처리",
    "href": "posts/07wk.html#결측치-처리",
    "title": "07wk: 전처리: 데이터 형태 갖추기 - 전처리 총 정리",
    "section": "결측치 처리",
    "text": "결측치 처리\n\ndf.isna().sum()\n\ngender                 0\nage                    0\nhypertension           0\nheart_disease          0\never_married           0\nwork_type              0\nResidence_type         0\navg_glucose_level      0\nbmi                  201\nsmoking_status         0\nstroke                 0\ndtype: int64\n\n\n\n[col for col in df.columns if df[col].isnull().sum() &gt; 0]\n\n['bmi']\n\n\n\nX = df.drop([\"stroke\"], axis=1)\ny = LabelEncoder().fit_transform(df['stroke'])\n\n\nX.head()\n\n\n\n\n\n\n\n\ngender\nage\nhypertension\nheart_disease\never_married\nwork_type\nResidence_type\navg_glucose_level\nbmi\nsmoking_status\n\n\n\n\n0\nMale\n67.0\nNo\nYes\nYes\nPrivate\nUrban\n228.69\n36.6\nformerly smoked\n\n\n1\nFemale\n61.0\nNo\nNo\nYes\nSelf-employed\nRural\n202.21\nNaN\nnever smoked\n\n\n2\nMale\n80.0\nNo\nYes\nYes\nPrivate\nRural\n105.92\n32.5\nnever smoked\n\n\n3\nFemale\n49.0\nNo\nNo\nYes\nPrivate\nUrban\n171.23\n34.4\nsmokes\n\n\n4\nFemale\n79.0\nYes\nNo\nYes\nSelf-employed\nRural\n174.12\n24.0\nnever smoked\n\n\n\n\n\n\n\n\ny\n\narray([1, 1, 1, ..., 0, 0, 0])\n\n\n\nX_num = X.select_dtypes(include = 'number')\nX_cat = X.select_dtypes(exclude = 'number')\n\nX[X_num.columns] = SimpleImputer(strategy=\"mean\").fit_transform(X_num)\nX[X_cat.columns] = SimpleImputer(strategy=\"most_frequent\").fit_transform(X_cat)\n\n\nX.head()\n\n\n\n\n\n\n\n\ngender\nage\nhypertension\nheart_disease\never_married\nwork_type\nResidence_type\navg_glucose_level\nbmi\nsmoking_status\n\n\n\n\n0\nMale\n67.0\nNo\nYes\nYes\nPrivate\nUrban\n228.69\n36.600000\nformerly smoked\n\n\n1\nFemale\n61.0\nNo\nNo\nYes\nSelf-employed\nRural\n202.21\n28.893237\nnever smoked\n\n\n2\nMale\n80.0\nNo\nYes\nYes\nPrivate\nRural\n105.92\n32.500000\nnever smoked\n\n\n3\nFemale\n49.0\nNo\nNo\nYes\nPrivate\nUrban\n171.23\n34.400000\nsmokes\n\n\n4\nFemale\n79.0\nYes\nNo\nYes\nSelf-employed\nRural\n174.12\n24.000000\nnever smoked\n\n\n\n\n\n\n\n\nColumnTransformer()를 이용하여 표준화 및 인코딩 처리\n\n\nscaler = StandardScaler()\n\nonehot = OneHotEncoder(drop = 'first', handle_unknown='ignore', sparse_output=False)\n\nct = ColumnTransformer([('scaler', scaler, X_num.columns),\n                        ('onehot', onehot, X_cat.columns)], \n                       remainder='passthrough', n_jobs=-1)\n\n\nct\n\nColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformerColumnTransformer(n_jobs=-1, remainder='passthrough',\n                  transformers=[('scaler', StandardScaler(),\n                                 Index(['age', 'avg_glucose_level', 'bmi'], dtype='object')),\n                                ('onehot',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore',\n                                               sparse_output=False),\n                                 Index(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object'))])scalerIndex(['age', 'avg_glucose_level', 'bmi'], dtype='object')StandardScalerStandardScaler()onehotIndex(['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type',\n       'Residence_type', 'smoking_status'],\n      dtype='object')OneHotEncoderOneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)remainderpassthroughpassthrough\n\n\n\ntrain, test set 분리\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n\nct.fit_transform(X_train)\n\narray([[ 0.20566087, -0.8199733 ,  0.53847936, ...,  0.        ,\n         1.        ,  0.        ],\n       [-1.25490055,  0.35207477, -1.02064076, ...,  0.        ,\n         1.        ,  0.        ],\n       [ 1.04659017,  0.09066209, -0.51811444, ...,  0.        ,\n         1.        ,  0.        ],\n       ...,\n       [ 0.78103355, -0.61137349,  0.93792335, ...,  1.        ,\n         0.        ,  0.        ],\n       [-0.54674956, -0.71302171, -0.37637625, ...,  0.        ,\n         0.        ,  0.        ],\n       [-1.65323548, -0.33736527, -0.87890256, ...,  0.        ,\n         0.        ,  0.        ]])\n\n\n\nct.transform(X_test)\n\narray([[ 0.86955242, -0.62065442,  0.75752929, ...,  0.        ,\n         1.        ,  0.        ],\n       [-0.01563631, -0.43415205,  0.56424994, ...,  0.        ,\n         1.        ,  0.        ],\n       [-0.90082505,  0.44974544, -0.02847341, ...,  0.        ,\n         0.        ,  1.        ],\n       ...,\n       [ 0.29417974, -0.31305809, -1.04641134, ...,  0.        ,\n         1.        ,  0.        ],\n       [-1.25490055, -0.96670029,  0.43539704, ...,  0.        ,\n         1.        ,  0.        ],\n       [ 0.33843918, -0.50862041,  0.65444697, ...,  1.        ,\n         0.        ,  0.        ]])"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML",
    "section": "",
    "text": "11wk: 예측 모델 훈련과 선택 - 평가 지표\n\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\n정초윤\n\n\n\n\n\n\n  \n\n\n\n\n02wk: 전처리: 데이터 형태 갖추기 - 결측치\n\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\n정초윤\n\n\n\n\n\n\n  \n\n\n\n\n05wk: 전처리: 데이터 형태 갖추기 - 차원 축소\n\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\n정초윤\n\n\n\n\n\n\n  \n\n\n\n\n05wk: 전처리: 데이터 형태 갖추기 - 클래스 불균형\n\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\n정초윤\n\n\n\n\n\n\n  \n\n\n\n\n12wk: 예측 - LogisticRegression\n\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\n정초윤\n\n\n\n\n\n\n  \n\n\n\n\n14wk: 예측 - XGBoost\n\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\n정초윤\n\n\n\n\n\n\n  \n\n\n\n\n01wk: 머신 러닝 교과서 - 파이토치편 소개\n\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\n정초윤\n\n\n\n\n\n\n  \n\n\n\n\n13wk: 예측 - RandomForest\n\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\n정초윤\n\n\n\n\n\n\n  \n\n\n\n\n16wk: 예측 - MLPClassifier\n\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\n정초윤\n\n\n\n\n\n\n  \n\n\n\n\n15wk: 예측 - XGBoost-earlystopping\n\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\n정초윤\n\n\n\n\n\n\n  \n\n\n\n\n09wk: 예측 모델 훈련과 선택 - 파라미터 튜닝\n\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\n정초윤\n\n\n\n\n\n\n  \n\n\n\n\n10wk: 예측 모델 훈련과 선택 - 각 모델 튜닝 범위\n\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\n정초윤\n\n\n\n\n\n\n  \n\n\n\n\n03wk: 전처리: 데이터 형태 갖추기 - 범주형 인코딩\n\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\n정초윤\n\n\n\n\n\n\n  \n\n\n\n\n04wk: 전처리: 데이터 형태 갖추기 - scaler\n\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\n정초윤\n\n\n\n\n\n\n  \n\n\n\n\n07wk: 전처리: 데이터 형태 갖추기 - 전처리 총 정리\n\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\n정초윤\n\n\n\n\n\n\n  \n\n\n\n\n08wk: 예측 모델 훈련과 선택 - 교차 검증\n\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\n정초윤\n\n\n\n\n\n\nNo matching items"
  }
]